%%writefile app.py
import streamlit as st
import subprocess
import csv
import sys
import os
import pandas as pd
import json
import argparse
import torch
import torch.nn as nn

# --- (â˜…ìˆ˜ì •â˜…) íŽ˜ì´ì§€ ë ˆì´ì•„ì›ƒ ì„¤ì • ---
# 'wide' ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì„¤ì •í•˜ê³  íŽ˜ì´ì§€ ì œëª©(ë¸Œë¼ìš°ì € íƒ­)ì„ ì§€ì •í•©ë‹ˆë‹¤.
# ì´ ì½”ë“œëŠ” í•­ìƒ st.xxx ëª…ë ¹ì–´ ì¤‘ ê°€ìž¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
st.set_page_config(layout="wide", page_title="ì•…ì„± íŠ¸ëž˜í”½ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

# --- (â˜…ì¶”ê°€â˜…) ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.title("ET-BERT")
    st.write("---")
    # (ì•„ì´ì½˜ì€ streamlitì´ ì§€ì›í•˜ëŠ” ì´ëª¨ì§€ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤)
    # ë§í¬ë¥¼ '#'ë¡œ ì„¤ì •í•˜ì—¬ í˜„ìž¬ íŽ˜ì´ì§€ì— ë¨¸ë¬´ë¥´ë„ë¡ í•©ë‹ˆë‹¤.
    st.link_button("ëŒ€ì‹œë³´ë“œ", "#", icon="ðŸ ", use_container_width=True)
    st.link_button("ê²½ê³ ", "#", icon="âš ï¸", use_container_width=True)
    st.link_button("ì„¤ì •", "#", icon="âš™ï¸", use_container_width=True)
# --- ì‚¬ì´ë“œë°” ë ---

# --- (A) ET-BERT ëª¨ë¸ ë¡œë“œ ë¡œì§ (main.pyì—ì„œ ê°€ì ¸ì˜´) ---
# ì´ ë¡œì§ì€ ET-BERT ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.

# 1. "uer" ë¶€í’ˆ ë¡œë“œ ë¡œì§
repo_path = "/content/drive/MyDrive/ET-BERT-main/" # Colab ê²½ë¡œ ê¸°ì¤€
if repo_path not in sys.path:
    sys.path.append(repo_path)

try:
    from uer.layers import *
    from uer.encoders import *
    from uer.utils import *
    from uer.utils.vocab import Vocab
    from uer.utils.constants import *
except ImportError as e:
    st.error(f"Import ì˜¤ë¥˜: {e}. 'uer' í´ë” ê²½ë¡œ({repo_path})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# 2. "ëª¨ë¸ ë¼ˆëŒ€" (Classifier) ì •ì˜
class Classifier(torch.nn.Module):
    def __init__(self, args):
        super(Classifier, self).__init__()
        self.embedding = str2embedding[args.embedding](args, args.vocab_size)
        self.encoder = str2encoder[args.encoder](args)
        self.labels_num = args.labels_num
        self.pooling = args.pooling
        self.soft_targets = args.soft_targets
        self.soft_alpha = args.soft_alpha
        self.output_layer_1 = nn.Linear(args.hidden_size, args.hidden_size)
        self.output_layer_2 = nn.Linear(args.hidden_size, self.labels_num)

    def forward(self, src, seg):
        emb = self.embedding(src, seg)
        output = self.encoder(emb, seg)
        if self.pooling == "mean":
            output = torch.mean(output, dim=1)
        elif self.pooling == "max":
            output = torch.max(output, dim=1)[0]
        elif self.pooling == "last":
            output = output[:, -1,]
        else:
            output = output[:, 0, :]
        output = torch.tanh(self.output_layer_1(output))
        logits = self.output_layer_2(output)
        return logits

# 3. ëª¨ë¸/í† í¬ë‚˜ì´ì €/ì„¤ì •ê°’ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
# @st.cache_resource: ì´ í•¨ìˆ˜ëŠ” í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ê³  ê²°ê³¼ê°€ ìºì‹œë©ë‹ˆë‹¤.
@st.cache_resource
def load_all_resources():
    DRIVE_PATH = "/content/drive/MyDrive/ET-BERT-main/"
    MODEL_PATH = os.path.join(DRIVE_PATH, "fine-tuning/USTC-TFC_results/finetuned_model.bin")
    VOCAB_PATH = os.path.join(DRIVE_PATH, "models/encryptd_vocab.txt")
    CONFIG_PATH = os.path.join(DRIVE_PATH, "models/bert_base_config.json")

    try:
        args = argparse.Namespace()
        with open(CONFIG_PATH, "r") as f:
            config_dict = json.load(f)
            for key, value in config_dict.items():
                setattr(args, key, value)

        args.labels_num = 2
        vocab_size = 0
        with open(VOCAB_PATH, "r", encoding="utf-8") as f:
            for line in f:
                vocab_size += 1
        args.vocab_size = vocab_size

        # --- ëª¨ë“  ìˆ˜ë™ ì„¤ì •ê°’ ---
        args.pooling = "first"
        args.soft_targets = False
        args.soft_alpha = 0.5
        args.tokenizer = "bert"
        args.encoder = "transformer"
        args.mask = "fully_visible"
        args.embedding = "word_pos_seg"
        args.remove_embedding_layernorm = False
        args.parameter_sharing = False
        args.factorized_embedding_parameterization = False
        args.layernorm_positioning = "pre"
        args.remove_transformer_bias = False
        args.remove_attention_scale = False
        args.has_residual_attention = False
        args.relative_position_embedding = False
        args.feed_forward = "linear"
        args.layernorm = "normal"

        # --- ì´ë¦„ ë§¤í•‘ ---
        if hasattr(args, 'max_position_embeddings'):
            args.max_seq_length = args.max_position_embeddings
        else:
            args.max_seq_length = 512
        if hasattr(args, 'intermediate_size') and not hasattr(args, 'feedforward_size'):
            args.feedforward_size = args.intermediate_size
        if hasattr(args, 'num_attention_heads') and not hasattr(args, 'heads_num'):
            args.heads_num = args.num_attention_heads
        if hasattr(args, 'hidden_dropout_prob') and not hasattr(args, 'dropout'):
            args.dropout = args.hidden_dropout_prob
        if hasattr(args, 'hidden_size') and not hasattr(args, 'emb_size'):
            args.emb_size = args.hidden_size

        args.vocab_path = VOCAB_PATH
        args.spm_model_path = None
        args.config_path = CONFIG_PATH

        tokenizer = str2tokenizer[args.tokenizer](args)
        model = Classifier(args)

        model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')), strict=False)
        model.eval()

        print("--- ðŸ¥³ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ ---")
        return model, tokenizer, args

    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.stop()

# --- (B) ì‚¬ìš©ìžë‹˜ì˜ PCAP ë³€í™˜ í•¨ìˆ˜ ---
def pcap_to_tsv_tshark(input_pcap, output_tsv, fields):
    field_args = [arg for field in fields for arg in ('-e', field)]
    tshark_command = [
        'tshark',
        '-r', input_pcap,
        '-T', 'fields',
        '-E', 'separator=\t',
        '-E', 'header=y',
        *field_args
    ]

    try:
        process = subprocess.run(
            tshark_command,
            capture_output=True,
            text=True,
            check=True,
            encoding='utf-8'
        )
        with open(output_tsv, 'w', newline='', encoding='utf-8') as outfile:
            outfile.write(process.stdout)
        return True, None # ì„±ê³µ

    except FileNotFoundError:
        return False, "ì˜¤ë¥˜: 'tshark' ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Colabì—ì„œ !apt-get install -y tshark ë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”)"
    except subprocess.CalledProcessError as e:
        return False, f"TShark ì˜¤ë¥˜: {e.stderr}"
    except Exception as e:
        return False, f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}"

# --- (C) ET-BERT ëª¨ë¸ ì¶”ë¡  í•¨ìˆ˜ ---
def predict_hex_string(model, tokenizer, args, hex_string):
    seq_length = 128 # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ seq_length
    try:
        hex_string_spaced = " ".join([hex_string[i:i+2] for i in range(0, len(hex_string), 2)])
        src_ids = tokenizer.convert_tokens_to_ids([CLS_TOKEN] + tokenizer.tokenize(hex_string_spaced))
        seg_ids = [1] * len(src_ids)

        if len(src_ids) > seq_length:
            src_ids = src_ids[:seq_length]
            seg_ids = seg_ids[:seq_length]
        while len(src_ids) < seq_length:
            src_ids.append(0)
            seg_ids.append(0)

        input_tensor = torch.LongTensor([src_ids])
        segment_tensor = torch.LongTensor([seg_ids])

        with torch.no_grad():
            logits = model(input_tensor, segment_tensor)
            probabilities = torch.softmax(logits, dim=1)
            confidence, predicted_class = torch.max(probabilities, 1)

        label_map = {0: "Benign", 1: "Malicious"}
        label = label_map.get(predicted_class.item(), "Unknown")

        return label, confidence.item()

    except Exception as e:
        st.error(f"ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {e}")
        return "Error", 0.0

# --- (D) Streamlit UI ë©”ì¸ ë¡œì§ ---

# (â˜…ìˆ˜ì •â˜…) ì œëª© ë° ìƒíƒœ ë°°ì§€
col_title, col_status = st.columns([4, 1])
with col_title:
    st.title("ì•…ì„± íŠ¸ëž˜í”½ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.caption("ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„")
# with col_status:
    # (ìž„ì‹œ) ìƒíƒœ ë°°ì§€
    # st.error("ìƒíƒœ: â‘  ìœ„í—˜ ìƒíƒœ", icon="ðŸ”¥")

# 1. ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ (ìºì‹œë¨)
try:
    model, tokenizer, args = load_all_resources()
    st.success("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")
except Exception as e:
    # load_all_resources ë‚´ë¶€ì—ì„œ ì´ë¯¸ st.errorë¥¼ í˜¸ì¶œí•˜ì§€ë§Œ, ë§Œì•½ì„ ìœ„í•´ ì´ì¤‘ ì²´í¬
    st.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop() # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ì§€

# 2. íŒŒì¼ ì—…ë¡œë“œ UI (ëŒ€ì‹œë³´ë“œ ë ˆì´ì•„ì›ƒ ì¤‘ì•™ì— ë°°ì¹˜)
st.write("---")
# (â˜…ìˆ˜ì •â˜…) íŒŒì¼ ì—…ë¡œë”ë¥¼ ì¤‘ì•™ ì»¬ëŸ¼ì— ë°°ì¹˜í•˜ì—¬ ìŠ¤í¬ë¦°ìƒ·ê³¼ ìœ ì‚¬í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
col1_up, col2_up, col3_up = st.columns([1, 2, 1])
with col2_up:
    uploaded_file = st.file_uploader(
        "PCAP íŒŒì¼ì„ ì—¬ê¸°ì— ë“œëž˜ê·¸ ì•¤ ë“œë¡­í•˜ì„¸ìš”.",
        type=["pcap", "pcapng"],
        label_visibility="hidden" # "Drag and drop" í…ìŠ¤íŠ¸ê°€ ê¸°ë³¸ì´ë¯€ë¡œ ë¼ë²¨ ìˆ¨ê¹€
    )
st.write("---")

# 3. (â˜…ìˆ˜ì •â˜…) ê²°ê³¼ í‘œì‹œ: 3ë‹¨ ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ
if uploaded_file is not None:

    # 3-1. pcap ë³€í™˜
    INPUT_PCAP = "temp_uploaded.pcap"
    OUTPUT_TSV = "temp_output.tsv"
    with open(INPUT_PCAP, "wb") as f:
        f.write(uploaded_file.getbuffer())

    with st.spinner(f"{uploaded_file.name} íŒŒì¼ ë³€í™˜ ì¤‘... (tshark ì‹¤í–‰)"):
        # (â˜…ìˆ˜ì •â˜…) IP ì£¼ì†Œë„ í•¨ê»˜ ì¶”ì¶œí•©ë‹ˆë‹¤.
        desired_fields = ['ip.src', 'ip.dst', 'tcp.payload']
        success, error_msg = pcap_to_tsv_tshark(INPUT_PCAP, OUTPUT_TSV, desired_fields)

    if not success:
        st.error(error_msg)
    else:
        # 3-2. TSV íŒŒì¼ ì½ê¸°
        try:
            df = pd.read_csv(OUTPUT_TSV, sep='\t')
            if df.empty:
                 st.error("pcap íŒŒì¼ì—ì„œ ë¶„ì„í•  ìˆ˜ ìžˆëŠ” íŒ¨í‚·(TCP Payload)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                 st.stop()

            # 16ì§„ìˆ˜ ë° IP ì •ë³´ ì¶”ì¶œ
            hex_string = str(df.iloc[0].get('tcp.payload', 'N/A')).replace(":", "")
            ip_src = str(df.iloc[0].get('ip.src', 'N/A'))
            ip_dst = str(df.iloc[0].get('ip.dst', 'N/A'))

            if hex_string == 'N/A':
                st.error("'tcp.payload' í•„ë“œë¥¼ TSVì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()

            # 3-3. ëª¨ë¸ ì¶”ë¡ 
            with st.spinner("ET-BERT ëª¨ë¸ì´ ì¶”ë¡  ì¤‘..."):
                label, confidence = predict_hex_string(model, tokenizer, args, hex_string)

            # 3-4. (â˜…ìˆ˜ì •â˜…) 3ë‹¨ ì»¬ëŸ¼ìœ¼ë¡œ ê²°ê³¼ í‘œì‹œ
            col1, col2, col3 = st.columns(3)

            # --- ì¹´ë“œ 1: ì „ì²´ ìœ„í—˜ ìˆ˜ì¤€ ---
            with col1:
                st.subheader("ì „ì²´ ìœ„í—˜ ìˆ˜ì¤€")
                if "Malicious" in label:
                    st.metric("íƒì§€ ê²°ê³¼", label, "ì‹¬ê°", delta_color="inverse")
                    st.caption("ë²”ë¡€: ðŸ”´ ì‹¬ê° ðŸŸ¡ ì£¼ì˜ ðŸŸ¢ ì•ˆì „")
                else:
                    st.metric("íƒì§€ ê²°ê³¼", label, "ì•ˆì „", delta_color="normal")
                    st.caption("ë²”ë¡€: ðŸ”´ ì‹¬ê° ðŸŸ¡ ì£¼ì˜ ðŸŸ¢ ì•ˆì „")

                st.write("ì‹ ë¢°ë„ (ê²Œì´ì§€):")
                st.progress(confidence)


            # --- ì¹´ë“œ 2: ì£¼ìš” íƒì§€ ìœ„í˜‘ ---
            with col2:
                st.subheader("ì£¼ìš” íƒì§€ ìœ„í˜‘ (ì˜ˆìƒ)")
                if "Malicious" in label:
                    # (ì´ ë°ì´í„°ëŠ” ET-BERTê°€ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ *ê°€ì§œ* ë°ì´í„°ìž…ë‹ˆë‹¤)
                    threat_data = {
                        "Threat": ["Trojan.Zeus", "C&C ì„œë²„", "Botnet"],
                        "Percentage": [confidence * 0.85, confidence * 0.65, confidence * 0.48]
                    }
                    # Create DataFrame, set index for better bar chart labels
                    df_threat = pd.DataFrame(threat_data).set_index("Threat")
                    st.bar_chart(df_threat, y="Percentage") # Use y= to match screenshot's vertical bars
                else:
                    st.info("íƒì§€ëœ ì£¼ìš” ìœ„í˜‘ ì—†ìŒ.")

            # --- ì¹´ë“œ 3: ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ---
            with col3:
                st.subheader("ë„¤íŠ¸ì›Œí¬ ì •ë³´")
                st.text(f"ì¶œë°œì§€ IP: {ip_src}")
                st.text(f"ëª©ì ì§€ IP: {ip_dst}")
                st.write("---")
                st.metric("ëª¨ë¸ ì‹ ë¢°ë„", f"{confidence*100:.1f}%")

        except pd.errors.EmptyDataError:
            st.error("ìƒì„±ëœ TSV íŒŒì¼ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. pcap íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
        except Exception as e:
            st.error(f"TSV íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            st.error(traceback.format_exc())
